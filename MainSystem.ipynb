{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "# import face_recognition\n",
    "import numpy as np\n",
    "import joblib\n",
    "import uuid\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.layers import Flatten\n",
    "import imutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIZE = 224\n",
    "SIZE = 240"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "DetectorLabels = ['Mask','No Mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = []\n",
    "for directory_path in glob.glob(r\"D:\\ML DEEP LEARNING FACE\\Masked Face Recognition\\database\\Train\\*\"):\n",
    "    label = directory_path.split(\"\\\\\")[-1]\n",
    "    train_labels.append(label)\n",
    "train_labels = np.array(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Geetansh', 'Kamya', 'Varsha'], dtype='<U8')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data and labels into respective lists\n",
    "train_images = []\n",
    "train_labels = []\n",
    "\n",
    "for directory_path in glob.glob(\"database/Train/*\"):\n",
    "    label = directory_path.split(\"\\\\\")[-1]\n",
    "    # print(label)\n",
    "    for img_path in glob.glob(os.path.join(directory_path, \"*.jpg\")):\n",
    "        # print(img_path)\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_COLOR)       \n",
    "        img = cv2.resize(img, (SIZE, SIZE))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "        train_images.append(img)\n",
    "        train_labels.append(label)\n",
    "        \n",
    "train_images = np.array(train_images)\n",
    "train_labels = np.array(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode labels from text to integers.\n",
    "from sklearn import preprocessing\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "encoder.fit(train_labels)\n",
    "train_labels_encoded = encoder.transform(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = train_images, train_labels_encoded\n",
    "# Normalize pixel values to between 0 and 1\n",
    "x_train = x_train / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading model\n",
    "VGG_model = VGG16(weights='imagenet', include_top=False, input_shape=(SIZE, SIZE, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 240, 240, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 240, 240, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 240, 240, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 120, 120, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 120, 120, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 120, 120, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 60, 60, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 60, 60, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 60, 60, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 60, 60, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 30, 30, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 30, 30, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 30, 30, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 30, 30, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 15, 15, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 15, 15, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 15, 15, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 15, 15, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 0\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for layer in VGG_model.layers:\n",
    "\tlayer.trainable = False\n",
    "    \n",
    "x=Flatten()(VGG_model.output)\n",
    "VGG_model = Model(inputs=VGG_model.input, outputs=x)\n",
    "\n",
    "VGG_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 14s 6s/step\n"
     ]
    }
   ],
   "source": [
    "# feature_extractor=VGG_model.predict(x_train)\n",
    "features=VGG_model.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RANDOM FOREST\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "RF_model = RandomForestClassifier(n_estimators = 100, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(n_estimators=120, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(n_estimators=120, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(n_estimators=120, random_state=42)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RF_model.fit(features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_and_predict_mask(frame, faceNet, maskNet):\n",
    "\t# grab the dimensions of the frame and then construct a blob\n",
    "\t# from it\n",
    "\t(h, w) = frame.shape[:2]\n",
    "\tblob = cv2.dnn.blobFromImage(frame, 1.0, (224, 224),\n",
    "\t\t(104.0, 177.0, 123.0))\n",
    "\n",
    "\t# pass the blob through the network and obtain the face detections\n",
    "\tfaceNet.setInput(blob)\n",
    "\tdetections = faceNet.forward()\n",
    "\tprint(detections.shape)\n",
    "\n",
    "\t# initialize our list of faces, their corresponding locations,\n",
    "\t# and the list of predictions from our face mask network\n",
    "\tfaces = []\n",
    "\tlocs = []\n",
    "\tpreds = []\n",
    "\n",
    "\t# loop over the detections\n",
    "\tfor i in range(0, detections.shape[2]):\n",
    "\t\t# extract the confidence (i.e., probability) associated with\n",
    "\t\t# the detection\n",
    "\t\tconfidence = detections[0, 0, i, 2]\n",
    "\n",
    "\t\t# filter out weak detections by ensuring the confidence is\n",
    "\t\t# greater than the minimum confidence\n",
    "\t\tif confidence > 0.5:\n",
    "\t\t\t# compute the (x, y)-coordinates of the bounding box for\n",
    "\t\t\t# the object\n",
    "\t\t\tbox = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "\t\t\t(startX, startY, endX, endY) = box.astype(\"int\")\n",
    "\n",
    "\t\t\t# ensure the bounding boxes fall within the dimensions of\n",
    "\t\t\t# the frame\n",
    "\t\t\t(startX, startY) = (max(0, startX), max(0, startY))\n",
    "\t\t\t(endX, endY) = (min(w - 1, endX), min(h - 1, endY))\n",
    "\n",
    "\t\t\t# extract the face ROI, convert it from BGR to RGB channel\n",
    "\t\t\t# ordering, resize it to 224x224, and preprocess it\n",
    "\t\t\tface = frame[startY:endY, startX:endX]\n",
    "\t\t\tface = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\n",
    "\t\t\tface = cv2.resize(face, (224, 224))\n",
    "\t\t\tface = img_to_array(face)\n",
    "\t\t\tface = preprocess_input(face)\n",
    "\n",
    "\t\t\t# add the face and bounding boxes to their respective\n",
    "\t\t\t# lists\n",
    "\t\t\tfaces.append(face)\n",
    "\t\t\tlocs.append((startX, startY, endX, endY))\n",
    "\n",
    "\t# only make a predictions if at least one face was detected\n",
    "\tif len(faces) > 0:\n",
    "\t\t# for faster inference we'll make batch predictions on *all*\n",
    "\t\t# faces at the same time rather than one-by-one predictions\n",
    "\t\t# in the above `for` loop\n",
    "\t\tfaces = np.array(faces, dtype=\"float32\")\n",
    "\t\tpreds = maskNet.predict(faces, batch_size=32)\n",
    "\n",
    "\t# return a 2-tuple of the face locations and their corresponding\n",
    "\t# locations\n",
    "\treturn (locs, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "protoPath = \"D:\\\\ML DEEP LEARNING FACE\\\\Masked Face Recognition\\\\face_detector\\\\deploy.prototxt\"\n",
    "weightsPath = \"D:\\\\ML DEEP LEARNING FACE\\\\Masked Face Recognition\\\\face_detector\\\\res10_300x300_ssd_iter_140000.caffemodel\"\n",
    "\n",
    "faceNet = cv2.dnn.readNet(protoPath, weightsPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "maskNet = load_model('D:\\\\ML DEEP LEARNING FACE\\\\Masked Face Recognition\\\\face_detector\\\\mask_detector.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG_model.save('featuresVGG.h5')\n",
    "joblib.dump(RF_model, \"RFModel.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vrgpv\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator DecisionTreeClassifier from version 1.1.2 when using version 1.1.3. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\Users\\vrgpv\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator RandomForestClassifier from version 1.1.2 when using version 1.1.3. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "VGG_model = load_model(\"D:\\\\ML DEEP LEARNING FACE\\\\Masked Face Recognition\\\\featuresVGG.h5\")\n",
    "RF_model = joblib.load(\"D:\\\\ML DEEP LEARNING FACE\\\\Masked Face Recognition\\\\RFModel.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('model_vgg16Sparse.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(img):\n",
    "    if img is None:\n",
    "        return 'Unknown'\n",
    "    img = cv2.resize(img,(SIZE,SIZE))\n",
    "    input_img = np.expand_dims(img, axis=0) #Expand dims so the input is (num images, x, y, c)\n",
    "    prediction_RF=model.predict(input_img)\n",
    "    prediction_RF = np.argmax(prediction_RF, axis=1)\n",
    "    prediction_RF = encoder.inverse_transform([prediction_RF]) if prediction_RF<3 else None\n",
    "    return str(prediction_RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 729ms/step\n",
      "['Geetansh']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vrgpv\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\preprocessing\\_label.py:154: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "img = cv2.imread(r'D:\\ML DEEP LEARNING FACE\\Masked Face Recognition\\database\\Test\\Geetansh\\IMG_20220903_224600.jpg')\n",
    "print(predict(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 200, 7)\n",
      "1/1 [==============================] - 1s 858ms/step\n",
      "1/1 [==============================] - 0s 233ms/step\n",
      "(1, 1, 200, 7)\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 271ms/step\n",
      "(1, 1, 200, 7)\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 211ms/step\n",
      "(1, 1, 200, 7)\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 204ms/step\n",
      "(1, 1, 200, 7)\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 201ms/step\n",
      "(1, 1, 200, 7)\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 200ms/step\n",
      "(1, 1, 200, 7)\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 230ms/step\n",
      "(1, 1, 200, 7)\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 224ms/step\n",
      "(1, 1, 200, 7)\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 203ms/step\n",
      "(1, 1, 200, 7)\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 205ms/step\n",
      "(1, 1, 200, 7)\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 247ms/step\n",
      "(1, 1, 200, 7)\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 236ms/step\n",
      "(1, 1, 200, 7)\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 233ms/step\n",
      "(1, 1, 200, 7)\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 202ms/step\n",
      "(1, 1, 200, 7)\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 199ms/step\n",
      "(1, 1, 200, 7)\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 201ms/step\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "\t_ , frame = cap.read()\n",
    "\n",
    "\t# detect faces with mask or not\n",
    "\t(locs, preds) = detect_and_predict_mask(frame, faceNet, maskNet)\n",
    "\n",
    "\t# loop over the detected face locations\n",
    "\tfor (box, pred) in zip(locs, preds):\n",
    "\t\t(startX, startY, endX, endY) = box\n",
    "\t\t(mask, withoutMask) = pred\n",
    "\n",
    "\t\tlabel = \"Mask\" if mask > withoutMask else \"No Mask\"\n",
    "\t\tcolor = (0, 255, 0) if label == \"Mask\" else (0, 0, 255)\n",
    "\n",
    "\t\tlabel = \"{}: {:.2f}%\".format(label, max(mask, withoutMask) * 100)\n",
    "\n",
    "\t\tcv2.putText(frame, label, (startX, startY - 10),cv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 2)\n",
    "\t\tcv2.rectangle(frame, (startX, startY), (endX, endY), color, 2)\n",
    "\t\tcv2.rectangle(frame, (startX, startY), (endX, endY-120), (0,0,0), 2)\n",
    "\t\timg = frame[startY:endY-120,startX:endX]\n",
    "\t\t\n",
    "\t\t# cv2.imshow(\"cut\",img)\n",
    "\t\tname = predict(img)\n",
    "\t\tif img is not None:\n",
    "\t\t\tcv2.putText(frame, name, (endX-20, endY+10),cv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 2)\n",
    "  \n",
    "\tcv2.imshow(\"Frame\", frame)\n",
    "\tkey = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "\tif key == ord(\"q\"):\n",
    "\t\tbreak\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "featureDetector = load_model(r'face_detector\\featuresVGGDetector.h5')\n",
    "RFDetector = joblib.load(r'face_detector\\RFModelDetector.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect(img):\n",
    "    if img is None:\n",
    "        return None\n",
    "    img = cv2.resize(img,(224,224))\n",
    "    input_img = np.expand_dims(img, axis=0) #Expand dims so the input is (num images, x, y, c)\n",
    "    input_img_feature=featureDetector.predict(input_img)\n",
    "    prediction_RF = RFDetector.predict(input_img_feature)[0]\n",
    "    prediction = RFDetector.predict_proba(input_img_feature)\n",
    "    return DetectorLabels[prediction_RF],prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 488ms/step\n",
      "No Mask\n",
      "0.78\n"
     ]
    }
   ],
   "source": [
    "img = cv2.imread(r'D:\\ML DEEP LEARNING FACE\\Masked Face Recognition\\data\\MaskImages\\without_mask\\0_0_caizhuoyan_0014.jpg')\n",
    "# max = np.max(predict(img))\n",
    "label, predictions = detect(img)\n",
    "print(label)\n",
    "print(np.max(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 177ms/step\n",
      "1/1 [==============================] - 0s 236ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vrgpv\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\preprocessing\\_label.py:154: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 179ms/step\n",
      "1/1 [==============================] - 0s 208ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vrgpv\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\preprocessing\\_label.py:154: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 189ms/step\n",
      "1/1 [==============================] - 0s 212ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vrgpv\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\preprocessing\\_label.py:154: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 185ms/step\n",
      "1/1 [==============================] - 0s 204ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vrgpv\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\preprocessing\\_label.py:154: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 182ms/step\n",
      "1/1 [==============================] - 0s 202ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vrgpv\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\preprocessing\\_label.py:154: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 180ms/step\n",
      "1/1 [==============================] - 0s 204ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vrgpv\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\preprocessing\\_label.py:154: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 184ms/step\n",
      "1/1 [==============================] - 0s 207ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vrgpv\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\preprocessing\\_label.py:154: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 175ms/step\n",
      "1/1 [==============================] - 0s 198ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vrgpv\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\preprocessing\\_label.py:154: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 185ms/step\n",
      "1/1 [==============================] - 0s 208ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vrgpv\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\preprocessing\\_label.py:154: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 181ms/step\n",
      "1/1 [==============================] - 0s 200ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vrgpv\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\preprocessing\\_label.py:154: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 184ms/step\n",
      "1/1 [==============================] - 0s 201ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vrgpv\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\preprocessing\\_label.py:154: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 184ms/step\n",
      "1/1 [==============================] - 0s 207ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vrgpv\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\preprocessing\\_label.py:154: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 192ms/step\n",
      "1/1 [==============================] - 0s 199ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vrgpv\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\preprocessing\\_label.py:154: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 178ms/step\n",
      "1/1 [==============================] - 0s 198ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vrgpv\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\preprocessing\\_label.py:154: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 184ms/step\n",
      "1/1 [==============================] - 0s 200ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vrgpv\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\preprocessing\\_label.py:154: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 180ms/step\n",
      "1/1 [==============================] - 0s 207ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vrgpv\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\preprocessing\\_label.py:154: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 188ms/step\n",
      "1/1 [==============================] - 0s 207ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vrgpv\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\preprocessing\\_label.py:154: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 272ms/step\n",
      "1/1 [==============================] - 0s 218ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vrgpv\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\preprocessing\\_label.py:154: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "cascade_classifier = cv2.CascadeClassifier('databaseGenerator\\haarcascade_frontalface_default.xml')\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret,frame=cap.read()\n",
    "    gray = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "    detection = cascade_classifier.detectMultiScale(gray,1.3,5)\n",
    "#     cv2.imshow('frame',frame)\n",
    "    \n",
    "    for d in detection:\n",
    "        (x,y,w,h)=d\n",
    "        frame = cv2.rectangle(frame,(x,y),(x+w,y+h),(0,255,0),2)\n",
    "        img_crop=frame[y:y+h,x:x+w]\n",
    "        # img_crop=cv2.resize(img_crop,(255,255))\n",
    "        label, predictions = detect(img_crop)\n",
    "        img = frame[y:y+h-120,x:x+w]\n",
    "        name = predict(img)\n",
    "        \n",
    "        color = (0, 255, 0) if label == \"Mask\" else (0, 0, 255)\n",
    "        label = \"{}: {:.2f}%\".format(label, np.max(predictions) * 100)\n",
    "\n",
    "        cv2.putText(frame, label, (x, y - 10),cv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 2)\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), color, 2)\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h-120), (0,0,0), 2)\n",
    "        cv2.putText(frame, name, (x+w-20, y+h+10),cv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 2)\n",
    "        \n",
    "    cv2.imshow('frame',frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0XFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
